{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predição de Temperatura com GRU - TinyML\n",
    "\n",
    "**Projeto:** Sistema de Predição de Temperatura para Microcontroladores\n",
    "\n",
    "**Objetivo:** Prever a temperatura do sensor AHT20 em 5, 10 e 15 minutos no futuro\n",
    "\n",
    "**Sensores utilizados:**\n",
    "- AHT20: Temperatura e Umidade\n",
    "- BMP280: Temperatura e Pressão\n",
    "\n",
    "**Modelo:** GRU (Gated Recurrent Unit) compatível com TensorFlow Lite Micro\n",
    "\n",
    "**Deploy:** Raspberry Pi Pico (RP2040)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carregar os Dados\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas necessárias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow e Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GRU, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Sklearn para pré-processamento e métricas\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Configuração de visualização\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Configurar seed para reprodutibilidade\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "print(f'TensorFlow: {tf.__version__}')\n",
    "print(f'Seed configurado: {SEED}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carregar o dataset de temperatura em lê csv\n",
    "df_raw = pd.read_csv('data/temp.csv')\n",
    "df_raw['Timestamp'] = pd.to_datetime(df_raw['Timestamp'])#converter coluna de timestamp para datetime\n",
    "print(f'Shape original: {df_raw.shape}')\n",
    "print(f'Colunas disponíveis: {list(df_raw.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecionar apenas as features de interesse\n",
    "features = ['Temp_AHT20_C', 'Umid_AHT20_pct', 'Temp_BMP280_C', 'Press_BMP280_hPa']\n",
    "df = df_raw[['Timestamp'] + features].copy()\n",
    "\n",
    "print(f'\\nDataset filtrado:')\n",
    "print(f'Shape: {df.shape}')\n",
    "print(f'Features selecionadas: {features}')\n",
    "print(f'Período dos dados: {df[\"Timestamp\"].min()} até {df[\"Timestamp\"].max()}')\n",
    "print(f'\\nPrimeiras linhas:')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pré-processamento dos Dados\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar df_clean \n",
    "df_clean = df.copy()\n",
    "\n",
    "#: aplicar remoção de outliers \n",
    "for col in features:\n",
    "    Q1 = df_clean[col].quantile(0.25)\n",
    "    Q3 = df_clean[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
    "\n",
    "print(f'Depois: {len(df_clean)} amostras')\n",
    "removed = len(df) - len(df_clean)\n",
    "print(f'Removidos: {removed} ({100*removed/len(df):.2f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Criar Sequências Temporais\n",
    "\n",
    "**Conceito:** Transformar dados tabulares em sequências 3D para GRU\n",
    "\n",
    "**Formato:**\n",
    "- Input (X): Janela de 10 timesteps com 4 features cada\n",
    "- Output (y): Temperatura futura em 3 horizontes (5, 10, 15 minutos)\n",
    "\n",
    "**Exemplo:**\n",
    "```\n",
    "X = [timestep_0, timestep_1, ..., timestep_9]  <- 10 timesteps passados\n",
    "y = [temp_t+10, temp_t+19, temp_t+29]           <- 3 previsões futuras\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configurar parâmetros das sequências\n",
    "WINDOW_SIZE = 10 #Tamanho janela\n",
    "HORIZONS = [10, 19, 29] # tempo para cada previsao (5 minutos, 10 minutos, 15 minutos)\n",
    "print(f'  - Prever temperatura em {HORIZONS[0] * 31 / 60:.1f}, {HORIZONS[1] * 31 / 60:.1f} e {HORIZONS[2] * 31 / 60:.1f} minutos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, window_size, horizons):\n",
    "    \"\"\"\n",
    "    Criar sequências temporais para treinamento\n",
    "    \n",
    "    Parâmetros:\n",
    "    - data: DataFrame com os dados\n",
    "    - window_size: tamanho da janela de observação\n",
    "    - horizons: lista de horizontes de previsão\n",
    "\n",
    "    Retorna:\n",
    "    - X: array 3D (samples, timesteps, features) -> Amostras e janelas temporais\n",
    "    - y: array 2D (samples, horizons) -> TARGET É APENAS OS VALORES DE PREVISAO PARA 5, 10 E 15 MINUTOS\n",
    "    \"\"\"\n",
    "    X, y = [], [] #armazenar x e y\n",
    "    target = 'Temp_AHT20_C'\n",
    "    \n",
    "    for i in range(len(data) - window_size - max(horizons)): #itera pelos dados, parando antes do final para garantir espaço suficiente\n",
    "        #para a janela + horizonte máximo de previsão\n",
    "        X.append(data.iloc[i:i+window_size][features].values) #Seleciona apenas as colunas de features \n",
    "        #Pega uma janela de tamanho window_size começando na posição i\n",
    "        y.append([data.iloc[i+window_size+h][target] for h in horizons])#Para cada horizonte em horizons (ex: [0, 5, 10]), pega o valor do target no futuro\n",
    "        #i+window_size+h = posição após a janela + horizonte\n",
    "        #Cria uma lista com múltiplos targets (previsões para diferentes tempos futuros)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(df_clean, WINDOW_SIZE, HORIZONS) #APLICA FUNÇÃO\n",
    "\n",
    "print(f'\\nX shape: {X.shape}')\n",
    "print(f'  - {X.shape[0]} amostras')\n",
    "print(f'  - {X.shape[1]} timesteps por amostra')\n",
    "print(f'  - {X.shape[2]} features por timestep')\n",
    "print(f'\\ny shape: {y.shape}')\n",
    "print(f'  - {y.shape[0]} amostras')\n",
    "print(f'  - {y.shape[1]} horizontes de previsão')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Divisão em Treino, Validação e Teste\n",
    "\n",
    "**Divisão:**\n",
    "- Treino: 70% dos dados (para treinar o modelo)\n",
    "- Validação: 15% dos dados (para ajustar hiperparâmetros)\n",
    "- Teste: 15% dos dados (para avaliar desempenho final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividir dados em treino, validação e teste\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.85 * len(X))\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:val_size], y[train_size:val_size]\n",
    "X_test, y_test = X[val_size:], y[val_size:]\n",
    "\n",
    "print('Dados divididos:')\n",
    "print(f'  Treino:    {len(X_train):6d} amostras ({100*len(X_train)/len(X):.1f}%)')\n",
    "print(f'  Validação: {len(X_val):6d} amostras ({100*len(X_val)/len(X):.1f}%)')\n",
    "print(f'  Teste:     {len(X_test):6d} amostras ({100*len(X_test)/len(X):.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Normalização dos Dados\n",
    "\n",
    "**Por que normalizar?**\n",
    "- Features têm escalas diferentes (temperatura ~20°C, pressão ~920 hPa)\n",
    "- Redes neurais aprendem melhor com dados normalizados\n",
    "- Evita que features com valores grandes dominem o treinamento\n",
    "\n",
    "**Método:** StandardScaler (Z-score normalization)\n",
    "- Transforma dados para média = 0 e desvio padrão = 1\n",
    "- Fórmula: (x - média) / desvio_padrão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()# Normalizar dados usando StandardScaler\n",
    "\n",
    "X_train_reshaped = X_train.reshape(-1, X_train.shape[-1]) #Transforma dados 3D (amostras, timesteps, features) em 2D (amostras*timesteps, features)\n",
    "#Necessário porque StandardScaler trabalha com dados 2D\n",
    "scaler.fit(X_train_reshaped) #Calcula média e desvio padrão de cada feature usando APENAS dados de treino\n",
    "#Aprende os parâmetros de normalização\n",
    "\n",
    "#aplica (x - média) / desvio para cada feature\n",
    "# Processo: reshape para 2D → normaliza → reshape de volta para 3D\n",
    "#Os MESMOS parâmetros aprendidos no treino são aplicados em val e test\n",
    "X_train_scaled = scaler.transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "X_val_scaled = scaler.transform(X_val.reshape(-1, X_val.shape[-1])).reshape(X_val.shape)\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "print(f'\\nParâmetros do scaler:')\n",
    "print(f'  Média (treino): {X_train_scaled.mean():.6f}')\n",
    "print(f'  Desvio (treino): {X_train_scaled.std():.6f}')\n",
    "print(f'\\nMédias por feature:')\n",
    "for i, name in enumerate(features):\n",
    "    print(f'  {name:20s}: mean={scaler.mean_[i]:8.4f}, std={scaler.scale_[i]:8.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Construção e Treinamento do Modelo\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Arquitetura do Modelo GRU\n",
    "\n",
    "**Por que GRU?**\n",
    "- Rede recorrente projetada para capturar dependências temporais de longo prazo\n",
    "- Mais leve que LSTM (2 gates vs 3 gates), ideal para TinyML\n",
    "- Processa a sequência timestep a timestep mantendo estado oculto (memória)\n",
    "- Compatível com TensorFlow Lite Micro\n",
    "\n",
    "**Arquitetura:**\n",
    "```\n",
    "Input (10 timesteps × 4 features)\n",
    "    |\n",
    "GRU(24 unidades) + Dropout(0.2)\n",
    "    |\n",
    "Dense(16) + ReLU + Dropout(0.2)\n",
    "    |\n",
    "Dense(3) [Output: 3 previsões]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir modelo GRU\n",
    "model = Sequential([\n",
    "    # Camada de entrada\n",
    "    Input(shape=(WINDOW_SIZE, 4)),  #Entrada: janela temporal com 4 features\n",
    "    \n",
    "    # Camada GRU - processa a sequência temporal mantendo estado oculto\n",
    "    # return_sequences=False: retorna apenas o último estado oculto (vetor de 24 valores)\n",
    "    GRU(24,\n",
    "        kernel_regularizer=regularizers.l2(0.0001),#Regularização L2 para evitar overfitting\n",
    "        recurrent_regularizer=regularizers.l2(0.0001),#Regularização nos pesos recorrentes\n",
    "        name='gru_1'),\n",
    "    \n",
    "    #Dropout para reduzir overfitting (desliga 20% dos neurônios)\n",
    "    Dropout(0.2, name='dropout_1'),\n",
    "    \n",
    "    # Camada densa para combinar features extraídas pela GRU\n",
    "    Dense(16, activation='relu',\n",
    "          kernel_regularizer=regularizers.l2(0.0001),\n",
    "          name='dense_1'),\n",
    "    \n",
    "    #Segundo dropout\n",
    "    Dropout(0.2, name='dropout_2'),\n",
    "    \n",
    "    #Camada de saída - 3 valores (previsões para 3 horizontes temporais)\n",
    "    Dense(3, activation='linear', name='output')\n",
    "], name='GRU_Temperature')\n",
    "\n",
    "# Compilar modelo\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0005),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print('\\nArquitetura:')\n",
    "model.summary()\n",
    "\n",
    "print(f'\\nTotal de parâmetros: {model.count_params():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Configurar Callbacks\n",
    "\n",
    "**Callbacks** são funções que executam durante o treinamento:\n",
    "\n",
    "1. **EarlyStopping**: Para o treinamento se a validação não melhorar\n",
    "2. **ReduceLROnPlateau**: Reduz learning rate quando estagnar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50, #aguarda 50 epocas sem melhora no val_loss antes de parar\n",
    "        restore_best_weights=True, #restaura pesos da melhor epoca\n",
    "        verbose=1#mostra quando para\n",
    "    ), \n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5, #reduz learning rate pela metade\n",
    "        patience=20, #aguarda 20 epoca sem melhorar antes de reduzir\n",
    "        min_lr=1e-7, #minimo\n",
    "        verbose=1 #mostrar quando reduzir\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Treinar o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit( #treinamento modelo\n",
    "    X_train_scaled, y_train, # x e y de treino sendo x o normalizado\n",
    "    validation_data=(X_val_scaled, y_val), #dados de validação\n",
    "    epochs=300, #300 epoca\n",
    "    batch_size=512,#Pega 512 amostras do dataset, gradiente mais estavel\n",
    "    callbacks=callbacks, #Callbacks mostrado acima\n",
    "    verbose=1 #mostra andamento\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Visualizar Curvas de Aprendizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar pasta images/GRU se não existir\n",
    "os.makedirs('images/GRU', exist_ok=True)\n",
    "\n",
    "# Plotar curvas de loss e MAE\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "axes[0].plot(history.history['loss'], label='Treino', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validação', linewidth=2)\n",
    "axes[0].set_xlabel('Época', fontsize=12)\n",
    "axes[0].set_ylabel('Loss (MSE)', fontsize=12)\n",
    "axes[0].set_title('Curva de Loss', fontweight='bold', fontsize=14)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history.history['mae'], label='Treino', linewidth=2)\n",
    "axes[1].plot(history.history['val_mae'], label='Validação', linewidth=2)\n",
    "axes[1].set_xlabel('Época', fontsize=12)\n",
    "axes[1].set_ylabel('MAE (°C)', fontsize=12)\n",
    "axes[1].set_title('Curva de MAE', fontweight='bold', fontsize=14)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/GRU/01_curvas_aprendizado.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Avaliação do Modelo\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Métricas no Conjunto de Teste\n",
    "\n",
    "**Métricas utilizadas:**\n",
    "- **MAE** (Mean Absolute Error): Erro médio absoluto em °C\n",
    "- **RMSE** (Root Mean Squared Error): Raiz do erro quadrático médio\n",
    "- **R²** (Coefficient of Determination): Quanto o modelo explica a variância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_scaled, verbose=0)#Fazer predições no conjunto de teste\n",
    "# Calcular métricas gerais no conjunto de teste\n",
    "mae_overall = mean_absolute_error(y_test, y_pred)\n",
    "rmse_overall = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2_overall = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calcular MAE por horizonte\n",
    "mae_t1 = mean_absolute_error(y_test[:, 0], y_pred[:, 0]) #5 minutos\n",
    "mae_t2 = mean_absolute_error(y_test[:, 1], y_pred[:, 1])#10 minutos\n",
    "mae_t3 = mean_absolute_error(y_test[:, 2], y_pred[:, 2]) #15 minutos\n",
    "\n",
    "print(f'\\nMétricas Gerais:')\n",
    "print(f'  MAE:  {mae_overall:.4f} °C')\n",
    "print(f'  RMSE: {rmse_overall:.4f} °C')\n",
    "print(f'  R²:   {r2_overall:.4f}')\n",
    "print(f'\\nMAE por Horizonte:')\n",
    "print(f'  T+10 (5 min):   {mae_t1:.4f} °C')\n",
    "print(f'  T+19 (10 min):  {mae_t2:.4f} °C')\n",
    "print(f'  T+29 (15 min):  {mae_t3:.4f} °C')\n",
    "print(f'\\nInformações do Modelo:')\n",
    "print(f'  Parâmetros totais: {model.count_params():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Gráficos de Predito vs Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar scatter plots para cada horizonte\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "horizons_names = ['T+10 (5 min)', 'T+19 (10 min)', 'T+29 (15 min)']\n",
    "\n",
    "for i, (horizon_name, ax) in enumerate(zip(horizons_names, axes)):\n",
    "    ax.scatter(y_test[:, i], y_pred[:, i], alpha=0.4, s=15)\n",
    "    min_val = min(y_test[:, i].min(), y_pred[:, i].min())\n",
    "    max_val = max(y_test[:, i].max(), y_pred[:, i].max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Ideal')\n",
    "    ax.set_xlabel('Real (°C)', fontsize=12)\n",
    "    ax.set_ylabel('Predito (°C)', fontsize=12)\n",
    "    ax.set_title(horizon_name, fontweight='bold', fontsize=14)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    mae = mean_absolute_error(y_test[:, i], y_pred[:, i])\n",
    "    r2 = r2_score(y_test[:, i], y_pred[:, i])\n",
    "    ax.text(0.05, 0.95, f'MAE: {mae:.3f}°C\\nR²: {r2:.3f}',\n",
    "            transform=ax.transAxes, va='top', fontsize=11,\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/GRU/02_scatter_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Gráfico salvo: images/GRU/02_scatter_predictions.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Séries Temporais de Predições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar série temporal das predições\n",
    "n_samples = 300\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
    "\n",
    "for i, (horizon_name, ax) in enumerate(zip(horizons_names, axes)):\n",
    "    ax.plot(y_test[:n_samples, i], label='Real', linewidth=2, alpha=0.8)\n",
    "    ax.plot(y_pred[:n_samples, i], label='Predito', linewidth=2, alpha=0.8)\n",
    "    ax.set_ylabel('Temperatura (°C)', fontsize=12)\n",
    "    ax.set_title(horizon_name, fontweight='bold', fontsize=14)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[2].set_xlabel('Amostra', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/GRU/03_timeseries_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Gráfico salvo: images/GRU/03_timeseries_predictions.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Análise de Erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular erros\n",
    "errors = y_pred - y_test\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, (horizon_name, ax) in enumerate(zip(horizons_names, axes)):\n",
    "    ax.hist(errors[:, i], bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Erro zero')\n",
    "    ax.set_xlabel('Erro (°C)', fontsize=12)\n",
    "    ax.set_ylabel('Frequência', fontsize=12)\n",
    "    ax.set_title(f'{horizon_name}: Distribuição dos Erros', fontweight='bold', fontsize=14)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    mean_err = errors[:, i].mean()\n",
    "    std_err = errors[:, i].std()\n",
    "    ax.text(0.05, 0.95, f'Média: {mean_err:.3f}\\nStd: {std_err:.3f}',\n",
    "            transform=ax.transAxes, va='top', fontsize=11,\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/GRU/04_error_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Gráfico salvo: images/GRU/04_error_distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conversão para TensorFlow Lite\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Salvar Modelo Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar pasta models/GRU se não existir\n",
    "os.makedirs('models/GRU', exist_ok=True)\n",
    "\n",
    "model.save('models/GRU/temperature_model.keras')\n",
    "print('Modelo Keras salvo: models/GRU/temperature_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Converter para TensorFlow Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter para TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('models/GRU/temperature_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "size_kb = len(tflite_model) / 1024\n",
    "print(f'Modelo TFLite salvo: models/GRU/temperature_model.tflite')\n",
    "print(f'Tamanho: {size_kb:.2f} KB')\n",
    "\n",
    "if size_kb < 100:\n",
    "    print('Compatível com Raspberry Pi Pico (2MB Flash)')\n",
    "else:\n",
    "    print('AVISO: Modelo grande para microcontrolador')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Verificar Conversão TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar modelo TFLite\n",
    "interpreter = tf.lite.Interpreter(model_path='models/GRU/temperature_model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print('Informações do modelo TFLite:')\n",
    "print(f'  Input shape:  {input_details[0][\"shape\"]}')\n",
    "print(f'  Input dtype:  {input_details[0][\"dtype\"]}')\n",
    "print(f'  Output shape: {output_details[0][\"shape\"]}')\n",
    "print(f'  Output dtype: {output_details[0][\"dtype\"]}')\n",
    "\n",
    "test_sample = X_test_scaled[0:1].astype(np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], test_sample)\n",
    "interpreter.invoke()\n",
    "tflite_pred = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "keras_pred = model.predict(test_sample, verbose=0)\n",
    "\n",
    "print(f'\\nTeste de conversão:')\n",
    "print(f'  Predição Keras:  {keras_pred[0]}')\n",
    "print(f'  Predição TFLite: {tflite_pred[0]}')\n",
    "print(f'  Diferença máxima: {np.abs(keras_pred - tflite_pred).max():.6f} °C')\n",
    "\n",
    "if np.abs(keras_pred - tflite_pred).max() < 0.01:\n",
    "    print('\\nConversão TFLite: OK')\n",
    "else:\n",
    "    print('\\nAVISO: Diferença significativa após conversão')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Geração de Arquivo C para RP2040\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Converter Modelo TFLite para C Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_c_array(tflite_model, var_name='model_data'):#Cria um array de bytes em formato C\n",
    "    c_array = f\"const unsigned char {var_name}[] = {{\\n\"#array de bytes constante (fica na memória flash)\n",
    "    for i in range(0, len(tflite_model), 12): # # Percorre de 12 em 12 bytes\n",
    "        line = \"  \"\n",
    "        for j in range(12):## Coloca 12 bytes por linha\n",
    "            if i + j < len(tflite_model):\n",
    "                line += f\"0x{tflite_model[i+j]:02x}, \"## Formato hexadecimal\n",
    "        c_array += line + \"\\n\"\n",
    "    c_array += \"};\\n\"\n",
    "    c_array += f\"const unsigned int {var_name}_len = {len(tflite_model)};\\n\"\n",
    "    return c_array\n",
    "#Criação do arquivo .h:\n",
    "h_content = \"\"\"// Temperature Prediction Model - TinyML (GRU)\n",
    "// Auto-generated file - Do not edit manually\n",
    "// Model trained on AHT20 + BMP280 sensor data\n",
    "\n",
    "#ifndef TEMPERATURE_MODEL_H\n",
    "#define TEMPERATURE_MODEL_H\n",
    "\n",
    "// Model information\n",
    "#define WINDOW_SIZE 10\n",
    "#define NUM_FEATURES 4\n",
    "#define NUM_HORIZONS 3\n",
    "\n",
    "// Feature names\n",
    "const char* feature_names[] = {\n",
    "    \"Temp_AHT20_C\",\n",
    "    \"Umid_AHT20_pct\",\n",
    "    \"Temp_BMP280_C\",\n",
    "    \"Press_BMP280_hPa\"\n",
    "};\n",
    "\n",
    "// Horizon names\n",
    "const char* horizon_names[] = {\n",
    "    \"5 minutes\",\n",
    "    \"10 minutes\",\n",
    "    \"15 minutes\"\n",
    "};\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "h_content += convert_to_c_array(tflite_model, 'temperature_model')#Insere o array de bytes do modelo\n",
    "\n",
    "h_content += \"\"\"\n",
    "#endif // TEMPERATURE_MODEL_H\n",
    "\"\"\"\n",
    "\n",
    "h_filename = 'models/GRU/temperature_model.h'\n",
    "with open(h_filename, 'w') as f:\n",
    "    f.write(h_content)\n",
    "\n",
    "print(f'Arquivo {h_filename} gerado com sucesso!')\n",
    "print(f'Tamanho do modelo: {len(tflite_model)} bytes ({len(tflite_model)/1024:.2f} KB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Gerar Parâmetros do Scaler para C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar arquivo header com parâmetros de normalização\n",
    "scaler_h_content = \"\"\"// Scaler parameters for normalization\n",
    "// Auto-generated file - Do not edit manually\n",
    "\n",
    "#ifndef SCALER_PARAMS_H\n",
    "#define SCALER_PARAMS_H\n",
    "\n",
    "// Mean values\n",
    "const float scaler_mean[] = {\n",
    "\"\"\"\n",
    "\n",
    "for i, mean_val in enumerate(scaler.mean_):\n",
    "    scaler_h_content += f\"    {mean_val:.6f}f\"\n",
    "    if i < len(scaler.mean_) - 1:\n",
    "        scaler_h_content += \",\"\n",
    "    scaler_h_content += f\"  // {features[i]}\\n\"\n",
    "\n",
    "scaler_h_content += \"\"\"};\\n\n",
    "// Scale values\n",
    "const float scaler_scale[] = {\n",
    "\"\"\"\n",
    "\n",
    "for i, scale_val in enumerate(scaler.scale_):\n",
    "    scaler_h_content += f\"    {scale_val:.6f}f\"\n",
    "    if i < len(scaler.scale_) - 1:\n",
    "        scaler_h_content += \",\"\n",
    "    scaler_h_content += f\"  // {features[i]}\\n\"\n",
    "\n",
    "scaler_h_content += \"\"\"};\\n\n",
    "#endif // SCALER_PARAMS_H\n",
    "\"\"\"\n",
    "\n",
    "scaler_filename = 'models/GRU/scaler_params.h'\n",
    "with open(scaler_filename, 'w') as f:\n",
    "    f.write(scaler_h_content)\n",
    "\n",
    "print(f'Arquivo {scaler_filename} gerado com sucesso!')\n",
    "print(f'\\nParâmetros do Scaler:')\n",
    "print(f'  Mean: {scaler.mean_}')\n",
    "print(f'  Scale: {scaler.scale_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3. Salvar Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(scaler, 'models/GRU/scaler.pkl')\n",
    "print('Scaler salvo: models/GRU/scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Resumo Final\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('RESUMO DO PROJETO - GRU')\n",
    "print('='*70)\n",
    "\n",
    "print(f'\\nDataset:')\n",
    "print(f'  - Total amostras: {len(df_clean):,}')\n",
    "print(f'  - Features: {features}')\n",
    "print(f'  - Target: Temp_AHT20_C')\n",
    "print(f'  - Período: {df_clean[\"Timestamp\"].min()} até {df_clean[\"Timestamp\"].max()}')\n",
    "\n",
    "print(f'\\nModelo:')\n",
    "print(f'  - Arquitetura: GRU (1 camada GRU + Dense)')\n",
    "print(f'  - Parâmetros: {model.count_params():,}')\n",
    "print(f'  - Tamanho TFLite: {len(tflite_model)/1024:.2f} KB')\n",
    "print(f'  - Compatível: TensorFlow Lite Micro')\n",
    "\n",
    "print(f'\\nPerformance (Teste):')\n",
    "print(f'  - MAE Geral:    {mae_overall:.4f} °C')\n",
    "print(f'  - RMSE:         {rmse_overall:.4f} °C')\n",
    "print(f'  - R²:           {r2_overall:.4f}')\n",
    "print(f'  - MAE (5 min):  {mae_t1:.4f} °C')\n",
    "print(f'  - MAE (10 min): {mae_t2:.4f} °C')\n",
    "print(f'  - MAE (15 min): {mae_t3:.4f} °C')\n",
    "\n",
    "print(f'\\nArquivos Gerados:')\n",
    "print(f'  - models/GRU/temperature_model.keras')\n",
    "print(f'  - models/GRU/temperature_model.tflite')\n",
    "print(f'  - models/GRU/temperature_model.h')\n",
    "print(f'  - models/GRU/scaler_params.h')\n",
    "print(f'  - models/GRU/scaler.pkl')\n",
    "print(f'  - images/GRU/01-04.png')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('PROJETO GRU CONCLUÍDO COM SUCESSO!')\n",
    "print('='*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}